---
title: CAUSALab talk on self-supervised learning and the attention mechanism 
presentation_date: October 29, 2021
img: /assets/img/talks/embeddings.png
category: research talks
slides: true
slides_keynote: https://drive.google.com/file/d/1OtJB5sNmbbducyifoUYXFPTOxzpTqEeC/view?usp=sharing
slides_ppt: https://docs.google.com/presentation/d/1_Q36M0tuS9ZxkQNakd3CUGI_ArdlULik/edit?usp=sharing&ouid=107160263751202522175&rtpof=true&sd=true
slides_pdf: https://drive.google.com/file/d/1Jg7qQWO6VGCdbA3lco3OGpaxqiEpN379/view?usp=sharing
summary: A brief introduction for the CAUSALab to the self-supervised learning paradigm, attention-based models from deep learning, and the Transformer architecture. I begin by highlighting the fervent interest in causal inference (CI) from the machine learning (ML) community, which comes as a surprise to most researchers in epidemiology and CI. The longer-term goal is to invent methods for integrating causality into machine learning systems, with the <em>hard generalization problem</em> in mind. This is in contrast to the converse, integrating ML into CI, which has typically taken a much less ambitious form, such as fitting a propensity score model with a learning algorithm. 
# Can add a blog link to a relevant blog post here.
# Can also add a code link to relevant code here.
# Can even add a poster link.
---