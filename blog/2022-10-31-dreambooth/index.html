<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  David R. Bellamy


  | How to make an online avatar with deep learning

</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/monokai.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2022-10-31-dreambooth/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">David</span> R.  Bellamy
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          <li class="nav-item"><a class="nav-link" href="/assets/pdf/cv.pdf">cv</a></li> 
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">How to make an online avatar with deep learning</h1>
    <p class="post-meta">October 31, 2022</p>
  </header>

  <article class="post-content">
    <p><strong>TLDR;</strong> this tutorial is an extension to the instructions from <a href="https://github.com/XavierXiao/Dreambooth-Stable-Diffusion">Xavier Xiao’s repo</a> with some gaps filled and tips added.</p>

<p><strong>Summary</strong>: Since their release in December 2021, <a href="https://arxiv.org/abs/2112.10752">latent diffusion models (LDMs)</a> have completely taken over image synthesis. <a href="https://dreambooth.github.io/">DreamBooth</a>, released 2 months ago, allows us to fine-tune an LDM called <a href="https://stability.ai/blog/stable-diffusion-announcement">Stable Diffusion</a> on images of our choice and then generate related images via text prompts. We can use this approach to fine-tune Stable Diffusion on images of ourselves and then generate beautiful avatars.</p>

<h2 id="tutorial-steps">Tutorial Steps:</h2>
<ol>
  <li>Clone <a href="https://github.com/XavierXiao/Dreambooth-Stable-Diffusion">this repo</a> to your local development environment.
    <ul>
      <li>If you don’t have a local GPU, set up a remote sync with this repo and your GPU environment.</li>
    </ul>
  </li>
  <li>Set up a conda environment on your GPU server by running <code class="language-plaintext highlighter-rouge">conda env create -f environment.yaml</code>
    <ul>
      <li>Make sure that the custom taming-transformers and clip installations work; otherwise do them manually.</li>
    </ul>
  </li>
  <li>Download the diffusion model weights from Hugging Face <a href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/tree/main">here</a>.
    <ul>
      <li>To download them directly to your GPU environment, do it programmatically with:</li>
    </ul>
  </li>
</ol>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">huggingface_hub</span> <span class="kn">import</span> <span class="n">hf_hub_download</span>
<span class="nf">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="sh">"</span><span class="s">CompVis/stable-diffusion-v-1-4-original</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">filename</span><span class="o">=</span><span class="sh">"</span><span class="s">sd-v1-4-full-ema.ckpt</span><span class="sh">"</span><span class="p">,</span> <span class="n">use_auth_token</span><span class="o">=</span><span class="sh">'</span><span class="s">your_hf_auth_token</span><span class="sh">'</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<ul>
  <li>
    <p>Note: beware of the dash between ‘v’ and ‘1’ in the repo name, some sources don’t have it and it will lead to 404 Not Found errors. Also, you can create a Hugging Face authentication token in your Hugging Face profile’s settings under Access Tokens.</p>
  </li>
  <li>
    <p>The downloaded weights will end up in a directory of the form: ./models–CompVis–stable-diffusion-v-1-4-original/snapshots/0834a76f88354683d3f7ef271cadd28f4757a8cc/sd-v1-4-full-ema.ckpt. Use this path to reference the weights when fine-tuning the diffusion model.</p>
  </li>
</ul>

<ol start="4">
  <li>Gather your training images for fine-tuning the diffusion model.
    <ul>
      <li>If you want to fine-tune the model on yourself, the recommendation is:
        <ul>
          <li>2-3 full body</li>
          <li>3-5 upper body</li>
          <li>5-12 close-up on face</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Gather your regularization images, to avoid overfitting.
    <ul>
      <li>These should be images in the same ‘category’ as your training images, yet distinct. So if you are finetuning on yourself, gather images of other people.</li>
      <li>I recommend gathering these images from the internet rather than generating them from the diffusion model (as the Github repo suggests).</li>
      <li>The more regularization images you have, the better. 100+ is apparently ideal, but a lot of effort. I got away with just 8.</li>
    </ul>
  </li>
  <li>Finetune the diffusion model with this command</li>
</ol>

<figure class="highlight"><pre><code class="language-python" data-lang="python">   <span class="n">python</span> <span class="n">main</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">base</span> <span class="n">configs</span><span class="o">/</span><span class="n">stable</span><span class="o">-</span><span class="n">diffusion</span><span class="o">/</span><span class="n">v1</span><span class="o">-</span><span class="n">finetune_unfrozen</span><span class="p">.</span><span class="n">yaml</span> 
                <span class="o">-</span><span class="n">t</span> 
                <span class="o">--</span><span class="n">actual_resume</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">sd</span><span class="o">-</span><span class="n">v1</span><span class="o">-</span><span class="mi">4</span><span class="o">-</span><span class="n">full</span><span class="o">-</span><span class="n">ema</span><span class="p">.</span><span class="n">ckpt</span>  
                <span class="o">-</span><span class="n">n</span> <span class="o">&lt;</span><span class="n">job</span> <span class="n">name</span><span class="o">&gt;</span> 
                <span class="o">--</span><span class="n">gpus</span> <span class="mi">0</span><span class="p">,</span> 
                <span class="o">--</span><span class="n">data_root</span> <span class="o">/</span><span class="n">root</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">training</span><span class="o">/</span><span class="n">images</span> 
                <span class="o">--</span><span class="n">reg_data_root</span> <span class="o">/</span><span class="n">root</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">regularization</span><span class="o">/</span><span class="n">images</span> 
                <span class="o">--</span><span class="n">class_word</span> <span class="o">&lt;</span><span class="n">xxx</span><span class="o">&gt;</span>
   </code></pre></figure>

<ul>
  <li>For the class_word, I recommend using a noun, not an adjective/style. Ex. do not use “portrait” (a style of image), use “man” or “woman” instead.</li>
  <li>On one A100, the training takes ~15 min or so.</li>
  <li>Note: the gpus ‘0’ argument is not referring to “no GPUs”, it refers to the device number.</li>
</ul>

<ol start="7">
  <li>Generate samples!</li>
</ol>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">stable_txt2img</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">ddim_eta</span> <span class="mf">0.0</span> 
                                 <span class="o">--</span><span class="n">n_samples</span> <span class="mi">8</span> 
                                 <span class="o">--</span><span class="n">n_iter</span> <span class="mi">1</span> 
                                 <span class="o">--</span><span class="n">scale</span> <span class="mf">10.0</span> 
                                 <span class="o">--</span><span class="n">ddim_steps</span> <span class="mi">100</span>  
                                 <span class="o">--</span><span class="n">seed</span> <span class="mi">1</span>
                                 <span class="o">--</span><span class="n">ckpt</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">saved</span><span class="o">/</span><span class="n">checkpoint</span><span class="o">/</span><span class="k">from</span><span class="o">/</span><span class="n">training</span>
                                 <span class="o">--</span><span class="n">prompt</span> <span class="sh">"</span><span class="s">photo of a sks &lt;class&gt;</span><span class="sh">"</span></code></pre></figure>

<ul>
  <li>Your checkpoint from training will be under a directory of the form: <code class="language-plaintext highlighter-rouge">logs/train_images2022-10-31T10-23-15_job1/checkpoints/last.ckpt</code></li>
  <li>Note: I added a seed argument so that you can change it if you wish to generate more samples using the same prompt as prior attempts; just change the seed.</li>
</ul>

<h2 id="tips-on-prompting">Tips on Prompting</h2>
<p>For prompting, I recommend looking through images at <a href="https://lexica.art/">lexica.art</a> and <a href="https://proximacentaurib.notion.site/e28a4f8d97724f14a784a538b8589e7d?v=ab624266c6a44413b42a6c57a41d828c">this Notion page</a>.</p>
<ul>
  <li>Find images you like and try their prompts yourself! You will also learn about good prompting structures/keywords, like wlop, loish, hyper realistic, etc. which you can use in prompts of your own design to better effect.</li>
</ul>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2023 David R. Bellamy.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
    Last updated: May 16, 2023.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
