<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml"/><link href="/" rel="alternate" type="text/html"/><updated>2023-12-22T23:04:02+00:00</updated><id>/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">My job search for summer 2023</title><link href="/blog/2022-11-01-jobsearch/" rel="alternate" type="text/html" title="My job search for summer 2023"/><published>2023-02-01T00:00:00+00:00</published><updated>2023-02-01T00:00:00+00:00</updated><id>/blog/jobsearch</id><content type="html" xml:base="/blog/2022-11-01-jobsearch/"><![CDATA[<p>I am graduating in May 2023 with my PhD in Epidemiology and Masters in Biostatistics, and I am searching for jobs beginning around June 1, 2023. I am broadly interested in Machine Learning Scientist, Researcher or Engineer roles in industry.</p> <p>Given my extensive background in deep learning, healthcare and the molecular sciences, I think I would be a great fit for roles at the intersection of these spaces, such as at <a href="https://www.nfx.com/post/biotech-to-techbio"><code class="language-plaintext highlighter-rouge">TechBio</code></a> companies like <a href="https://generatebiomedicines.com/"><code class="language-plaintext highlighter-rouge">Generate Biomedicines</code></a>, <a href="https://insitro.com/"><code class="language-plaintext highlighter-rouge">Insitro</code></a>, <a href="https://www.dynotx.com/"><code class="language-plaintext highlighter-rouge">Dyno Therapeutics</code></a>, and <a href="https://www.isomorphiclabs.com/"><code class="language-plaintext highlighter-rouge">Isomorphic Labs</code></a>, as well as research groups such as the Protein Team at Meta AI (led by <a href="https://twitter.com/alexrives"><code class="language-plaintext highlighter-rouge">Alex Rives</code></a>) and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-ai4science/"><code class="language-plaintext highlighter-rouge">AI4Science</code></a> at Microsoft Research.</p> <p>I am also very interested in teams that push the frontier of deep learning forward, such as those at OpenAI, Stability.AI, Anthropic, DeepMind, and Google Research. Finally, I would like productionization to be a component of my next job – i.e. I want to write production code. I am very interested in having a product focus drive my work, and I would love to learn more about early-stage companies and startups by working at one myself.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A specification of roles that interest me]]></summary></entry><entry><title type="html">How to make an online avatar with deep learning</title><link href="/blog/2022-10-31-dreambooth/" rel="alternate" type="text/html" title="How to make an online avatar with deep learning"/><published>2022-10-31T17:15:00+00:00</published><updated>2022-10-31T17:15:00+00:00</updated><id>/blog/dreambooth</id><content type="html" xml:base="/blog/2022-10-31-dreambooth/"><![CDATA[<p><strong>TLDR;</strong> this tutorial is an extension to the instructions from <a href="https://github.com/XavierXiao/Dreambooth-Stable-Diffusion">Xavier Xiao’s repo</a> with some gaps filled and tips added.</p> <p><strong>Summary</strong>: Since their release in December 2021, <a href="https://arxiv.org/abs/2112.10752">latent diffusion models (LDMs)</a> have completely taken over image synthesis. <a href="https://dreambooth.github.io/">DreamBooth</a>, released 2 months ago, allows us to fine-tune an LDM called <a href="https://stability.ai/blog/stable-diffusion-announcement">Stable Diffusion</a> on images of our choice and then generate related images via text prompts. We can use this approach to fine-tune Stable Diffusion on images of ourselves and then generate beautiful avatars.</p> <h2 id="tutorial-steps">Tutorial Steps:</h2> <ol> <li>Clone <a href="https://github.com/XavierXiao/Dreambooth-Stable-Diffusion">this repo</a> to your local development environment. <ul> <li>If you don’t have a local GPU, set up a remote sync with this repo and your GPU environment.</li> </ul> </li> <li>Set up a conda environment on your GPU server by running <code class="language-plaintext highlighter-rouge">conda env create -f environment.yaml</code> <ul> <li>Make sure that the custom taming-transformers and clip installations work; otherwise do them manually.</li> </ul> </li> <li>Download the diffusion model weights from Hugging Face <a href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/tree/main">here</a>. <ul> <li>To download them directly to your GPU environment, do it programmatically with:</li> </ul> </li> </ol> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">huggingface_hub</span> <span class="kn">import</span> <span class="n">hf_hub_download</span>
<span class="nf">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="sh">"</span><span class="s">CompVis/stable-diffusion-v-1-4-original</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">filename</span><span class="o">=</span><span class="sh">"</span><span class="s">sd-v1-4-full-ema.ckpt</span><span class="sh">"</span><span class="p">,</span> <span class="n">use_auth_token</span><span class="o">=</span><span class="sh">'</span><span class="s">your_hf_auth_token</span><span class="sh">'</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)</span></code></pre></figure> <ul> <li> <p>Note: beware of the dash between ‘v’ and ‘1’ in the repo name, some sources don’t have it and it will lead to 404 Not Found errors. Also, you can create a Hugging Face authentication token in your Hugging Face profile’s settings under Access Tokens.</p> </li> <li> <p>The downloaded weights will end up in a directory of the form: ./models–CompVis–stable-diffusion-v-1-4-original/snapshots/0834a76f88354683d3f7ef271cadd28f4757a8cc/sd-v1-4-full-ema.ckpt. Use this path to reference the weights when fine-tuning the diffusion model.</p> </li> </ul> <ol start="4"> <li>Gather your training images for fine-tuning the diffusion model. <ul> <li>If you want to fine-tune the model on yourself, the recommendation is: <ul> <li>2-3 full body</li> <li>3-5 upper body</li> <li>5-12 close-up on face</li> </ul> </li> </ul> </li> <li>Gather your regularization images, to avoid overfitting. <ul> <li>These should be images in the same ‘category’ as your training images, yet distinct. So if you are finetuning on yourself, gather images of other people.</li> <li>I recommend gathering these images from the internet rather than generating them from the diffusion model (as the Github repo suggests).</li> <li>The more regularization images you have, the better. 100+ is apparently ideal, but a lot of effort. I got away with just 8.</li> </ul> </li> <li>Finetune the diffusion model with this command</li> </ol> <figure class="highlight"><pre><code class="language-python" data-lang="python">   <span class="n">python</span> <span class="n">main</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">base</span> <span class="n">configs</span><span class="o">/</span><span class="n">stable</span><span class="o">-</span><span class="n">diffusion</span><span class="o">/</span><span class="n">v1</span><span class="o">-</span><span class="n">finetune_unfrozen</span><span class="p">.</span><span class="n">yaml</span> 
                <span class="o">-</span><span class="n">t</span> 
                <span class="o">--</span><span class="n">actual_resume</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">sd</span><span class="o">-</span><span class="n">v1</span><span class="o">-</span><span class="mi">4</span><span class="o">-</span><span class="n">full</span><span class="o">-</span><span class="n">ema</span><span class="p">.</span><span class="n">ckpt</span>  
                <span class="o">-</span><span class="n">n</span> <span class="o">&lt;</span><span class="n">job</span> <span class="n">name</span><span class="o">&gt;</span> 
                <span class="o">--</span><span class="n">gpus</span> <span class="mi">0</span><span class="p">,</span> 
                <span class="o">--</span><span class="n">data_root</span> <span class="o">/</span><span class="n">root</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">training</span><span class="o">/</span><span class="n">images</span> 
                <span class="o">--</span><span class="n">reg_data_root</span> <span class="o">/</span><span class="n">root</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">regularization</span><span class="o">/</span><span class="n">images</span> 
                <span class="o">--</span><span class="n">class_word</span> <span class="o">&lt;</span><span class="n">xxx</span><span class="o">&gt;</span>
   </code></pre></figure> <ul> <li>For the class_word, I recommend using a noun, not an adjective/style. Ex. do not use “portrait” (a style of image), use “man” or “woman” instead.</li> <li>On one A100, the training takes ~15 min or so.</li> <li>Note: the gpus ‘0’ argument is not referring to “no GPUs”, it refers to the device number.</li> </ul> <ol start="7"> <li>Generate samples!</li> </ol> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">stable_txt2img</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">ddim_eta</span> <span class="mf">0.0</span> 
                                 <span class="o">--</span><span class="n">n_samples</span> <span class="mi">8</span> 
                                 <span class="o">--</span><span class="n">n_iter</span> <span class="mi">1</span> 
                                 <span class="o">--</span><span class="n">scale</span> <span class="mf">10.0</span> 
                                 <span class="o">--</span><span class="n">ddim_steps</span> <span class="mi">100</span>  
                                 <span class="o">--</span><span class="n">seed</span> <span class="mi">1</span>
                                 <span class="o">--</span><span class="n">ckpt</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">saved</span><span class="o">/</span><span class="n">checkpoint</span><span class="o">/</span><span class="k">from</span><span class="o">/</span><span class="n">training</span>
                                 <span class="o">--</span><span class="n">prompt</span> <span class="sh">"</span><span class="s">photo of a sks &lt;class&gt;</span><span class="sh">"</span></code></pre></figure> <ul> <li>Your checkpoint from training will be under a directory of the form: <code class="language-plaintext highlighter-rouge">logs/train_images2022-10-31T10-23-15_job1/checkpoints/last.ckpt</code></li> <li>Note: I added a seed argument so that you can change it if you wish to generate more samples using the same prompt as prior attempts; just change the seed.</li> </ul> <h2 id="tips-on-prompting">Tips on Prompting</h2> <p>For prompting, I recommend looking through images at <a href="https://lexica.art/">lexica.art</a> and <a href="https://proximacentaurib.notion.site/e28a4f8d97724f14a784a538b8589e7d?v=ab624266c6a44413b42a6c57a41d828c">this Notion page</a>.</p> <ul> <li>Find images you like and try their prompts yourself! You will also learn about good prompting structures/keywords, like wlop, loish, hyper realistic, etc. which you can use in prompts of your own design to better effect.</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[A short tutorial on using Dreambooth to fine-tune stable diffusion]]></summary></entry></feed>